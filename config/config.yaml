global:
  DATA_PATH: "./data/"

chains:
  base:
    #Roles that A2rchi knows about
    ROLES: 
      - User
      - A2rchi
      - Expert 
  chain: 
    #Pick one of the models listed in the model class map below
    MODEL_NAME: OpenAILLM 
    #Map of all the class models and their keyword arguments
    MODEL_CLASS_MAP:
      OpenAILLM: 
        class: OpenAILLM
        kwargs: 
          model_name: gpt-4
          temperature: 1
      DumbLLM:
        class: DumbLLM
        kwargs: 
      LlamaLLM:
        class: LlamaLLM
        kwargs:
          base_model_path: "/data/submit/juliush/llama_models/Llama-2-7b-chat-hf/" #the location of the model (ex. meta-llama/Llama-2-70b)
          peft_model: null #the location of the finetuning of the model. Can be none
          enable_salesforce_content_safety: True # Enable safety check with Salesforce safety flan t5
          quantization: True #enables 8-bit quantization
          max_new_tokens: 4096 #The maximum numbers of tokens to generate
          seed: null #seed value for reproducibility
          do_sample: True #Whether or not to use sampling ; use greedy decoding otherwise.
          min_length: null #The minimum length of the sequence to be generated, input prompt + min_new_tokens
          use_cache: True  #[optional] Whether or not the model should use the past last key/values attentions Whether or not the model should use the past last key/values attentions (if applicable to the model) to speed up decoding.
          top_p: .9 # [optional] If set to float < 1, only the smallest set of most probable tokens with probabilities that add up to top_p or higher are kept for generation.
          temperature: .6 # [optional] The value used to modulate the next token probabilities.
          top_k: 50 # [optional] The number of highest probability vocabulary tokens to keep for top-k-filtering.
          repetition_penalty: 1.0 #The parameter for repetition penalty. 1.0 means no penalty.
          length_penalty: 1 #[optional] Exponential penalty to the length that is used with beam-based generation.
          max_padding_length: null # the max padding length to be used with tokenizer padding the prompts.
  input_lists:
    - submit.list

utils: 
  mailbox: 
    IMAP4_PORT: 143
  data_manager:
    CHUNK_SIZE: 1000
    CHUNK_OVERLAP: 0
    use_HTTP_chromadb_client: True
    chromadb_port: 8000
    chromadb_host: "localhost"
    collection_name: "dev_collection"
    reset_collection: True
  embeddings:
    #Choose one embedding from list below
    EMBEDDING_NAME: OpenAIEmbeddings
    #List of possible embeddings to use in vectorstore
    EMBEDDING_CLASS_MAP:
      OpenAIEmbeddings:
        class: OpenAIEmbeddings
        kwargs: 
          model: text-embedding-ada-002
        similarity_score_reference: 0.4
      HuggingFaceEmbeddings:
        class: HuggingFaceEmbeddings
        kwargs:
          model_name: "sentence-transformers/all-mpnet-base-v2"
          model_kwargs:
            device: 'cpu'
          encode_kwargs: 
            normalize_embeddings: False
        similarity_score_reference: 0.9
        
        
